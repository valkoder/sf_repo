{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7372616",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.events.off('checkpoint_created.Notebook');\n",
       "IPython.notebook.events.off('notebook_saved.Notebook');\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.events.off('checkpoint_created.Notebook');\n",
    "IPython.notebook.events.off('notebook_saved.Notebook');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96906fdd",
   "metadata": {},
   "source": [
    "###  Дмитрий МАРЬИН - группа dspr-58 - Car Price prediction   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f7bdd0",
   "metadata": {},
   "source": [
    "#### <span style=\"color: LightSeaGreen;\"> Юнит 7. Часть II - Проект 6. ВЫБИРАЕМ АВТО ВЫГОДНО</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d232f6b",
   "metadata": {},
   "source": [
    "моя страничка на GIT: https://github.com/valkoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fed6c76",
   "metadata": {},
   "source": [
    "это **вторая часть машинное обучение** - dmar_car_part_2.ipynb,  \n",
    "первая часть подготовка данных - в ноутбуке dmar_car_part_1.ipinb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbca57b9",
   "metadata": {},
   "source": [
    "<a name=\"top\"></a>  \n",
    "**Навигация:**\n",
    "- #### [setup](#import)\n",
    "- #### [исходные датасеты](#df)\n",
    "- #### [про метрику MAPE](#mape)\n",
    "- #### [baseline model - CatBoost regressor](#naive)\n",
    "- #### [особенности CatBoost](#cb)\n",
    "- #### [попытки улучшить рузультаты с CatBoost](#att1)\n",
    "- #### [другие модели](#models)  \n",
    "- #### [DecisionTreeRegressor](#dec_tree) - 20.50%, Kagle 17.04% my position 164\n",
    "- #### [ExtraTreesRegressor](#et) - **14.58%**, Kagle **12.55%** my posision 49\n",
    "- #### [RandomForestRegressor](#rf) - **12.71%** Kagle **13.56%**\n",
    "- #### [XGBoost](#xgb) - **13.32**, Kaggle **12.50%** my position 48\n",
    "- #### [GradientBoosting](#grad) - **12.50**, Kaggle **12.48%** my position 48  \n",
    "- #### [Stacking](#stack) - **12.97**, Kaggle **12.21%** my position **44**\n",
    "- #### [LINKS](#links)\n",
    "- #### [список литературы](#lit)\n",
    "- #### [to END](#end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639cdc70",
   "metadata": {},
   "source": [
    " <a name=\"import\"></a> \n",
    "#### setup и загрузка файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4a6aa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSION = XX\n",
    "R_S = 42 # random seed\n",
    "VAL_SIZE = 0.20 # validation part (X_test) in %\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from datetime import timedelta, datetime, date\n",
    "\n",
    "def display_all(df):     # display many columns / raws\n",
    "   with pd.option_context('display.max_rows', 6): # number of rows\n",
    "      with pd.option_context('display.max_columns', 35): # columns\n",
    "          display(df)\n",
    "\n",
    "def col_info(df, col):  # records, NaN, unique, duplicates\n",
    "    print(\"всего записей в столбце \", col, \" = \", \n",
    "          df[col].count())\n",
    "    print(\"NaN в столбце \",  col, \" = \", df[col].isna().sum(), \n",
    "          \",  в процентах = \", \n",
    "          \"{0:.0%}\".format(df[col].isna().sum() / len(df)))\n",
    "    print(\"уникальных в столбце \", col, \" = \", \n",
    "          df[col].nunique(dropna=False))\n",
    "    \n",
    "    a = df[df[col].duplicated(keep=False)].count()[1]    \n",
    "    print(\"всего записей-дубликатов в столбце \", col, \" = \", a)\n",
    "    \n",
    "    display_all(pd.DataFrame(df[col].value_counts(sort=True, dropna=False)))\n",
    "\n",
    "def info_table(df): # таблица NaN и unique по столбцам\n",
    "    mv = df.isnull().sum()\n",
    "    mvp = 100 * df.isnull().sum() / len(df)\n",
    "    mv2 = df.nunique(dropna=False)\n",
    "    mv_table = pd.concat([mv, mvp, mv2], axis=1)\n",
    "    mv_table2 = mv_table.rename(columns = {0: 'NaN', 1: '% NaN', 2: 'uniq'})\n",
    "    mv_table2 = mv_table2[mv_table2.iloc[:,1] >= 0].round(1)\n",
    "    return mv_table2\n",
    "def my_mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_pred-y_true)/y_true))\n",
    "# грузим файлы\n",
    "df_sub = pd.read_csv('empty_submission.csv')\n",
    "df = pd.read_csv('beta_200.csv') #see dmar_car_part_1.ipinb\n",
    "df_a = pd.read_csv('alpha_200.csv') #see dmar_car_part_1.ipinb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d46ef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# разметим категориальные столбцы - для CatBoost, для XGB - убрать\n",
    "cat_list = ['S01', 'S02', 'S03', 'S06', 'S10', \n",
    "           'S13', 'S14', 'S15', 'S16', 'S17', 'S21', 'S22']\n",
    "for col in cat_list:\n",
    "    df_a[col] = df_a[col].astype('category', errors='raise')\n",
    "    df[col] = df[col].astype('category', errors='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379c209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'] = df['price'].astype('int64', errors='raise')\n",
    "df['S05'] = df['S05'].astype('int64', errors='raise')\n",
    "df['S08'] = df['S08'].astype('int64', errors='raise')\n",
    "df['S16'] = df['S16'].astype('bool', errors='raise')\n",
    "df_a['S16'] = df_a['S16'].astype('bool', errors='raise')\n",
    "df['S17'] = df['S17'].astype('bool', errors='raise')\n",
    "df_a['S17'] = df_a['S17'].astype('bool', errors='raise')\n",
    "df['S21'] = df['S21'].astype('bool', errors='raise')\n",
    "df_a['S21'] = df_a['S21'].astype('bool', errors='raise')\n",
    "df['S22'] = df['S22'].astype('bool', errors='raise')\n",
    "df_a['S22'] = df_a['S22'].astype('bool', errors='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7b15348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45112 entries, 0 to 45111\n",
      "Data columns (total 20 columns):\n",
      " #   Column  Non-Null Count  Dtype   \n",
      "---  ------  --------------  -----   \n",
      " 0   S01     45112 non-null  category\n",
      " 1   S02     45112 non-null  category\n",
      " 2   S03     45112 non-null  category\n",
      " 3   S05     45112 non-null  int64   \n",
      " 4   S06     45112 non-null  category\n",
      " 5   S07     45112 non-null  int64   \n",
      " 6   S08     45112 non-null  int64   \n",
      " 7   S10     45112 non-null  category\n",
      " 8   S11     45112 non-null  int64   \n",
      " 9   S13     45112 non-null  category\n",
      " 10  S14     45112 non-null  category\n",
      " 11  S15     45112 non-null  category\n",
      " 12  S16     45112 non-null  bool    \n",
      " 13  S17     45112 non-null  bool    \n",
      " 14  S21     45112 non-null  bool    \n",
      " 15  S22     45112 non-null  bool    \n",
      " 16  S23     45112 non-null  int64   \n",
      " 17  S24     45112 non-null  float64 \n",
      " 18  S25     45112 non-null  float64 \n",
      " 19  price   45112 non-null  float64 \n",
      "dtypes: bool(4), category(8), float64(3), int64(5)\n",
      "memory usage: 3.3 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cf8e72",
   "metadata": {},
   "source": [
    "#### исходные датасеты <a name=\"df\"></a> \n",
    "(названия колонок в них одинаковые, кроме последних - 'sell_id' и 'price')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34accd2d",
   "metadata": {},
   "source": [
    "|поле |значение        |количество уникальных|\n",
    "|-----|----------------|---------------------|\n",
    "|S01\t|тип авто\t|24|\n",
    "|S02\t|производитель\t|12|\n",
    "|S03\t|цвет\t|16|\n",
    "|S05\t|мощность\t|314|\n",
    "|S06\t|топливо\t|5|\n",
    "|S07\t|пробег\t|> 10тыс|\n",
    "|S08\t|модельный возраст\t|69|\n",
    "|S10\t|двери\t|5|\n",
    "|S11\t|возраст авто\t|72|\n",
    "|S13\t|КПП\t|4|\n",
    "|S14\t|владельцы|\t3|\n",
    "|S15\t|привод\t|3|\n",
    "|S16\t|ПТС\t|2|\n",
    "|S17\t|руль\t|2|\n",
    "|S21\t|\"оптовик\"\t|2|\n",
    "|S22\t|\"торг\"\t|2|\n",
    "|S23\t|годовой пробег| |\t\n",
    "|S24\t|лог годового пробега\t| |\n",
    "|S25\t|лог мощности\t| |\n",
    "|S30\t|число слов| 1179|\t\n",
    "|S31\t|лог числа слов\t| |\n",
    "|price\t|price\t| |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2261a89",
   "metadata": {},
   "source": [
    "<a name=\"naive\"></a> \n",
    "#### модель из baseline - CatBoost regressor\n",
    "все ссылки лежат в разделе **[ссылки](#links)**   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567467fc",
   "metadata": {},
   "source": [
    "**Начальные результаты: MAPE: 13.04%, Kaggle 29%, my position 260**  \n",
    "без логарифмирования - 15.46%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb0ab687",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['price'], axis=1) # полный обучающий набор    \n",
    "y = df.price.values # значения для проверки обучающего\n",
    "Z = df_a.drop(['sell_id'], axis=1) # набор для submission\n",
    "# TRAIN TEST split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = VAL_SIZE, random_state = R_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1624c875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность по метрике MAPE: 13.02%\n"
     ]
    }
   ],
   "source": [
    "# Точность по метрике MAPE: 13.04%\n",
    "model = CatBoostRegressor(iterations = 5000,\n",
    "                          random_seed = R_S,\n",
    "                          eval_metric='MAPE',\n",
    "                          custom_metric=['R2', 'MAE'],\n",
    "                          silent=True)\n",
    "model.fit(X_train, \n",
    "          y = np.log(y_train),     # тут логарифм\n",
    "          cat_features=cat_list,\n",
    "          eval_set=(X_test, np.log(y_test) ),  # и тут логарифм\n",
    "          verbose_eval=0,\n",
    "          use_best_model = True,\n",
    "          plot=False)\n",
    "predict_new = np.exp(model.predict(X_test)) # тут возводим в СТЕПЕНЬ\n",
    "print(f\"Точность по метрике MAPE: {(my_mape(y_test, predict_new))*100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "552a175c",
   "metadata": {},
   "source": [
    "# вариант с логарифмированием отдельно от CatBoost\n",
    "# с логарифмированием MAPE: 13.04%\n",
    "# Score: 29.30769 позиция № 260  - плохо !\n",
    "\n",
    "# новый 'y_train_log' (с логарифмированием)\n",
    "y_train_log = np.log(y_train)\n",
    "y_test_log = np.log(y_test)\n",
    "\n",
    "model_log = CatBoostRegressor(iterations = 5000,\n",
    "                          random_seed = R_S,\n",
    "                          eval_metric='MAPE',\n",
    "                          custom_metric=['R2', 'MAE'],\n",
    "                          silent=True)\n",
    "model_log.fit(X_train, \n",
    "          y=y_train_log,\n",
    "          cat_features=cat_list,\n",
    "          eval_set=(X_test, y_test_log),\n",
    "          verbose_eval=0,  # лучше поправить в 1 или убрать\n",
    "          use_best_model=True\n",
    "          )\n",
    "predict_log = np.exp(model_log.predict(X_test)) # тут возводим в СТЕПЕНЬ\n",
    "print(f\"Точность MAPE: {(my_mape(y_test, predict_log))*100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bf6ad0",
   "metadata": {},
   "source": [
    "Результаты 13% и 29% на Каггле примерно такие-же как и в baselaine. Попробуем их улучшить."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b6de25",
   "metadata": {},
   "source": [
    "<a name=\"mape\"></a>  \n",
    "#### про метрику MAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4d11ba",
   "metadata": {},
   "source": [
    "MAPE (or MAPD mean absolute percentage deviation) \n",
    "- несимметрично реагирует на ошибку предсказания - по-разному в разные стороны  если модель ошибается в сторону занижения таргета, то mape не может превысить 100%, но если модель выдает завышенные предсказания, то у mape нет верхнего предела "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c85b4f",
   "metadata": {},
   "source": [
    "#### негативные результаты экспериментов с данными"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8230ad5",
   "metadata": {},
   "source": [
    "+ попытка использовать специальный тип данных pandas **ordered Categorical** dtype (from `pandas.api.types`) не удалась. \n",
    "CatBoostError: Invalid type for cat_feature ....  =NaN. Да и сама pandas похоже толком этот `CategoricalDtype` не поддердживает - тоже воспринимает его как NaN  \n",
    "+ созданные новые бинарные признаки 'оптовик' S21 и 'торг 'S22 на метрику не влияют  \n",
    "+ удаление из датасета второстепеных полей ('возраст модельного года' S08, 'топливо' S06, 'тип трансмиссии' S13, 'двери' S10' и т.п - на метрику не влияют\n",
    "+ сортировки данных в ключевых полях - на метрику не влияют\n",
    "+ создание выделение \"привлекательных\" категорий (групировка возраста авто на меньшее количество классов, выделение их по типу one-hot) - на метрику не влияют\n",
    "+ новые признаки полученные из колонки 'описание' - наличие в ней слов - 'ТО', 'гарантия', 'сервисная книжка', 'мимо'  - на метрику не влияют"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc95aef",
   "metadata": {},
   "source": [
    "<a name=\"cb\"></a>  \n",
    "#### особенности CatBoost  \n",
    "то немногое, что удалось найти\n",
    "+ Do not use one-hot encoding during preprocessing. This affects both the training speed and the resulting quality.\n",
    "+ CatBoost всеьма продвинутый пакет, у него свой \"интеллектуальный\" encoding сырых данных включая анализ данных и разные подходы к числовым, категориальным и текстовым, свой one-hot encoding, он сам создает новые фичи и их комбинации\n",
    "\n",
    "Получается, что предобработка и изощренный Feature Engineering ему вроде как не нужны"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03e1afe",
   "metadata": {},
   "source": [
    "<a name=\"att1\"></a>  \n",
    "#### попытки улучшить результаты с CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7263d81",
   "metadata": {},
   "source": [
    "**добавление поля S30 'счетчик слов' в 'description' и S31 его логарифма**):\n",
    "+  улучшило MAPE до **12.78%, на Kaggle 21.75%, my position 217** в leaderboard \n",
    "\n",
    "не реализовано:\n",
    "- общий пробег разделить на количество слов\n",
    "- продолжить работу с количеством слов - найти количество \"!\" (эмоции), кавычки (признак дилера), тире (манера письма)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bfb31e",
   "metadata": {},
   "source": [
    "#### CatBoost parameters to tune:\n",
    "|argument\t|description|\n",
    "|-----------|-----------|\n",
    "|iterations=500\t|max number of trees|\n",
    "|learning_rate=0.03\t|reduce gradient step, small -> more time|\n",
    "|depth=6\t|of the tree. can be 1-32. good 4 - 10|\n",
    "|l2_leaf_reg=3\t|values for the regularizer. any pos values|\n",
    "|border_count=32\t|# splits for num features. from 1 to 255|\n",
    "|ctr_border_count=50\t|# splits for cat. from 1 to 255 inclusively|"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3af26b37",
   "metadata": {},
   "source": [
    "CatBoost param optimization results ((iterations = 2000):\n",
    "2000 iter my orig = 13.05%\n",
    "depth: \n",
    "4=13.80, 6=13.05%, 7=12.81%, 10=12.39% ! 11=12.41% slow, 15=too slow, stopped\n",
    "best = 10\n",
    "\n",
    "learning_rate default= auto depending on iterations parameter\n",
    "no value = 13.46%\n",
    "0.03=13.66%, 0.06=13.02%, , 0.1=12.72%, 0.15=12.68%, 0.2= 12.74%\n",
    "off doc:By default, the value of the learning rate is defined automatically\n",
    "depending on the number of iterations and the input dataset\n",
    "\n",
    "l2_leaf_reg\n",
    "default=unknown\n",
    "no value=13.46% !, 0=15.52%, 1=15.66%, 2=15.75%, 3=15.84%, 8=16.15%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cf54a5",
   "metadata": {},
   "source": [
    "Ручной подбор параметро существенно картину не меняет.  \n",
    "Grid Search - для CatBoost долго, поэтому не использовался"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11930e0",
   "metadata": {},
   "source": [
    "#### библиотеки ML для задач регрессии  - выбор моделей\n",
    "+ линейные методы - Linear Regression, Ridge Regression, Lasso Regression, Gausian Regression, Polynomial Regression - не подходят к нашей задаче (сложные данные с не линейными отношениями (связями) между признаками\n",
    "+ Neural Network Regression, Deep Learning using Keras library в этом задании не используем\n",
    "+ KNN Model - хороший метод классификации, но не для нашей задачи\n",
    "+ Логистическая Регрессия - Logistic Regression  (S-curve based) врядли (будет огромное число классов), попробуем\n",
    "+ Decision Tree Regression, Random Forest, ансамблевые методы - будем применять далее\n",
    "+ Машины Опорных Векторов - Support Vector Machine не для нашей задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888527ad",
   "metadata": {},
   "source": [
    "<a name=\"models\"></a>  \n",
    "+ на бейслайне после обработки данных 13.04%, но на Кагле 29% и позиция 260\n",
    "#### проверяем модели  \n",
    "+ [DecisionTreeRegressor](#dec_tree) - 20.50%, Kagle 17.04% my position 164\n",
    "+ [ExtraTreesRegressor](#et) - **14.58%**, Kagle **12.55%** my posision **49**\n",
    "+ [RandomForestRegressor](#rf) - **12.71%** Kagle **13.56%**\n",
    "+ [XGBoost](#xgb) - **13.32**, Kaggle **12.50%** my position **48**\n",
    "+ [GradientBoosting](#grad) - **12.50**, Kaggle **12.48%** my position **48**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449f94f2",
   "metadata": {},
   "source": [
    "<a name=\"dec_tree\"></a>  \n",
    "#### DecisionTreeRegressor -  20.50%, Kagle 17.04% my pos 164"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d5f15a5",
   "metadata": {},
   "source": [
    "# DecisionTreeRegressor - по метрике MAPE: 20.50%\n",
    "# Kaggle = 17.04135 = 17% better then CatBoost 22% !\n",
    "VERSION = 18\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "clf = DecisionTreeRegressor(random_state=42, \n",
    "                            max_depth=None,\n",
    "                            max_features=None)\n",
    "model = clf.fit(X_train, y_train) \n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"по метрике MAPE: {(my_mape(y_test, y_pred))*100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "491dbbd1",
   "metadata": {},
   "source": [
    "# GridSearchCV для подбора гиперпараметров - НИЧЕГО НЕ ДАЛ\n",
    "# по метрике MAPE: 36.15% - ещё хуже\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'max_leaf_nodes': list(\n",
    "    range(2, 100)), 'min_samples_split': [2, 3, 4]}\n",
    "grid_search_cv = GridSearchCV(DecisionTreeRegressor(\n",
    "    random_state=42), params, verbose=1, cv=3)\n",
    "grid_search_cv.fit(X_train, y_train)\n",
    "grid_search_cv.best_estimator_\n",
    "y_pred = grid_search_cv.predict(X_test)\n",
    "print(f\"по метрике MAPE: {(my_mape(y_test, y_pred))*100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d92b2b3b",
   "metadata": {},
   "source": [
    "# DecisionTreeRegressor - по метрике MAPE: 20.50%\n",
    "# с логарифмированием - по метрике MAPE: 19.53% - тоже самое\n",
    "# Kaggle - Score: 17.26020 - тоже самое (без было - 17.04135)\n",
    "\n",
    "VERSION = 19\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "clf = DecisionTreeRegressor(random_state=42, \n",
    "                            max_depth=None,\n",
    "                            max_features=None)\n",
    "model = clf.fit(X_train, \n",
    "                y = np.log(y_train),     # тут логарифм\n",
    "                ) \n",
    "predict_new = np.exp(model.predict(X_test)) # тут возводим в СТЕПЕНЬ\n",
    "print(f\"Точность по метрике MAPE: {(my_mape(y_test, predict_new))*100:0.2f}%\")\n",
    "\n",
    "# предсказываем для submission\n",
    "predict_submission_draft = model.predict(Z)\n",
    "predict_submission = np.exp(predict_submission_draft) # тут возводим в СТЕПЕНЬ\n",
    "df_sub['price'] = predict_submission\n",
    "df_sub['price'] = df_sub['price'].apply(lambda x: round(x))\n",
    "df_sub.to_csv(f'submission_{VERSION}.csv', index=False) # пишем файл"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab64627d",
   "metadata": {},
   "source": [
    "**BaggingRegressor**  \n",
    "Код ниже обучает ансамбль из **500 деревьев** принятия решений, каждый из которых **обучается на 100 экземплярах**, случайно выбранных из обучающего набора **с заменой** (пример **бэггинга**,  \n",
    "но если вы хотите взамен применять **вставку**,  \n",
    "тогда просто установите `bootstrap=False`).  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "483edacc",
   "metadata": {},
   "source": [
    "#  MAPE 20%\n",
    "\n",
    "bag_clf = BaggingRegressor(\n",
    "    DecisionTreeRegressor(), n_estimators=500,\n",
    "    max_samples=100, \n",
    "    bootstrap=True, \n",
    "    # oob_score=True,\n",
    "    random_state=R_S)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "print(mean_absolute_percentage_error(y_test, y_pred))\n",
    "# 0.2927900887725549 - 29%\n",
    "\n",
    "\n",
    "tree_clf = DecisionTreeRegressor(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "print(mean_absolute_percentage_error(y_test, y_pred_tree))\n",
    "# 0.20500534668951267 - 20% "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b6e58e",
   "metadata": {},
   "source": [
    "Вместо построения экземпляра `BaggingRegressor` и его передачи экземпляру `DecisionTreeRegressor` можно применить специально созданный для этого класс  \n",
    "`RandomForestRegressor`, который является более удобным и оптимизированным\n",
    "для деревьев принятия решений   \n",
    "`BaggingREgressor` - использует  `DecisionTreeRegressor`!\n",
    "\n",
    "`BaggingClassifier`остается полезным, если нужен пакет чего-то, отличающегося от деревьев принятия решений."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90888120",
   "metadata": {},
   "source": [
    "Трудно сказать заранее, будет ли `RandomForestClassifier`\n",
    "выполняться лучше или хуже `ExtraTreesClassifier`.  \n",
    "\n",
    "Как правило, единственный способ узнать - попробовать оба и сравнить их с применением перекрестной проверки (подстраивая гиперпараметры с использованием решетчатого поиска).\n",
    "\n",
    "Начнем с `ExtraTreesRegressor `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35aa14c",
   "metadata": {},
   "source": [
    "<a name=\"et\"></a>  \n",
    "#### ExtraTreesRegressor - **14.58%**, Kagle **12.55%** my posision **49**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no log  - at home = MAPE: 14.58%  Kag = Score: 12.54712  now pos 49  \n",
    "model = ExtraTreesRegressor(random_state=R_S,\n",
    "                            n_estimators=100, # default 100\n",
    "                            # criterion='squared_error', # give error!\n",
    "                            max_depth=None,\n",
    "                            max_features='auto',\n",
    "                            bootstrap=False,\n",
    "                            oob_score=False,\n",
    "                            # n_jobs=None, # -1 error \n",
    "                            verbose=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"по метрике MAPE: {(my_mape(y_test, y_pred))*100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d869b424",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# most important features\n",
    "features = pd.Series(model.feature_importances_, index=X.columns)\n",
    "plt.subplots(figsize=(8, 3))\n",
    "features.nlargest(8).plot(kind='barh',grid=True)\n",
    "plt.xlabel('features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "44738597",
   "metadata": {},
   "source": [
    "#  -----<<<<<<<<<<   Why ?  score  >>>>>>--------\n",
    "# with log - at home = 12.6o%, Kaggle = ? scored 99.99738\n",
    "\n",
    "VERSION = 20\n",
    "model = ExtraTreesRegressor(random_state=R_S,\n",
    "                            n_estimators=100, # default 100\n",
    "                            # criterion='squared_error', # alt “absolute_error” error!\n",
    "                            max_depth=None,\n",
    "                            max_features='auto',\n",
    "                            bootstrap=False,\n",
    "                            oob_score=False,\n",
    "                            # n_jobs=None, # -1 error \n",
    "                            verbose=1)\n",
    "model.fit(X_train, np.log(y_train))\n",
    "y_pred = np.exp(model.predict(X_test))\n",
    "print(f\"по метрике MAPE: {(my_mape(y_test, y_pred))*100:0.2f}%\")\n",
    "\n",
    "# предсказываем для submission\n",
    "predict_submission = model.predict(Z)\n",
    "df_sub['price'] = predict_submission\n",
    "df_sub['price'] = df_sub['price'].apply(lambda x: round(x))\n",
    "df_sub.to_csv(f'submission_{VERSION}.csv', index=False) # пишем файл"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4192e9bf",
   "metadata": {},
   "source": [
    "<a name=\"rf\"></a>  \n",
    "#### RandomForestRegressor  -  **12.71%** Kagle **13.56%** "
   ]
  },
  {
   "cell_type": "raw",
   "id": "b222cd1e",
   "metadata": {},
   "source": [
    "# Score: 13.55573 - с логарифмированием - Точность MAPE: 12.71%\n",
    "model = RandomForestRegressor(n_estimators=100, \n",
    "                              verbose=1, \n",
    "                              n_jobs=-1, \n",
    "                              random_state=R_S,\n",
    "                              max_features=10,\n",
    "                              max_depth=18,\n",
    "                              min_samples_split=7\n",
    "                              )\n",
    "model.fit(X_train, np.log(y_train))\n",
    "predict_new = np.exp(model.predict(X_test)) # тут возводим в СТЕПЕНЬ\n",
    "print(f\"Точность по метрике MAPE: {(my_mape(y_test, predict_new))*100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f973049e",
   "metadata": {},
   "source": [
    "#### Voting Regressor \n",
    "\n",
    "на базе RandomForest, ExtraTrees и SVM не дает улучшения - 43%,  \n",
    "если выкинуть SVR получается лишь 14.52% и на Кагле - 13% (хуже чем просто ExtraTrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a9da05",
   "metadata": {},
   "source": [
    "<a name=\"xgb\"></a>  \n",
    "#### XGBoost - **13.32**, Kaggle **12.50%** my position **48**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "79eb527c",
   "metadata": {},
   "source": [
    "# по метрике MAPE: 13.32%   Score: 12.49859  position 49-> 48 \n",
    "# ver 4: 14.43% <<<<<<<<<  with LOG  = 13.32%    SUBMS =23  >>>>\n",
    "# alpha=10 -  14.50%\n",
    "# min_child_weight=1 -14.43%\n",
    "# min_child_weight=3 -15.45%\n",
    "# fix = max_depth=17 -14.43%  (or 12 ?)\n",
    "# fix = n_estimators=500, - 15.55%\n",
    "# n_estimators=1000, - 15.77% \n",
    "VERSION = 23\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', \n",
    "                         # enable_categorical=True,\n",
    "                         # use_label_encoder=True,\n",
    "                         max_depth=17, \n",
    "                         n_estimators=500, \n",
    "                         random_state=R_S, \n",
    "                         n_jobs=-1, \n",
    "                         )\n",
    "model.fit(X_train, np.log(y_train)) # здесь логарифм \n",
    "predict = np.exp(model.predict(X_test)) # тут возводим в СТЕПЕНЬ\n",
    "print(f\"Точность по метрике MAPE: {(my_mape(y_test, predict))*100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176da083",
   "metadata": {},
   "source": [
    "<a name=\"grad\"></a>  \n",
    "#### GradientBoosting - **12.48**, Kaggle **12.48%** my position **48**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d42820a",
   "metadata": {},
   "source": [
    "# Точность по метрике MAPE: 12.50%   Kag Score: 12.48354\n",
    "model = GradientBoostingRegressor(n_estimators=500, # 100\n",
    "                                  # loss='huber',   # 'squared_error'\n",
    "                                  max_features=12,   # None\n",
    "                                  max_depth=8,  #6\n",
    "                                  #learning_rate=  #0.1\n",
    "                                  random_state=R_S)\n",
    "model.fit(X_train, np.log(y_train))\n",
    "predict_new = np.exp(model.predict(X_test)) # тут возводим в СТЕПЕНЬ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae51268",
   "metadata": {},
   "source": [
    "Хороший результат после ручной настройки параметров. \n",
    "Поразительное совпадение метрик на обучающем наборе и на Кагде 12.50% vs 12.48%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee84b593",
   "metadata": {},
   "source": [
    "<a name=\"stack\"></a>  \n",
    "#### Stacking - **12.97**, Kaggle **12.21%** my position **44**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "121183de",
   "metadata": {},
   "source": [
    "простой стекинг без разбивки выборки на фолды дал следующие результаты:\n",
    "- ExtraTrees + DecisionTree = XGBR 13.32% --> 12.97%\n",
    "- RandomFor + GradientBoost = XGBR 13.32% --> 12.45%\n",
    "- RandomFor + ExtraTrees = XGBR 13.32% --> 12.85% \n",
    "- RandomFor + GradientBoost = ExtraTrees 14.50% --> 14.28%\n",
    "- RandomFor + XGBR 13.32% = DecisionTree 20.50% --> 19.45%\n",
    "\n",
    "то есть простая агрегация ответов нижних моделей верхней моделью не превосходит результаты отдельных моделей"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9073bd5c",
   "metadata": {},
   "source": [
    "# << DecisionTree 20.50% + ExtraTrees 14.50% => XGBR 13.32% >> 12.95%\n",
    "estimators = [('d_t', DecisionTreeRegressor(random_state=R_S, # 20.50%\n",
    "                            max_depth=None,\n",
    "                            max_features=None)\n",
    "               ),\n",
    "              \n",
    "              ('e_t', ExtraTreesRegressor(random_state=R_S, # 14.58%\n",
    "                            n_estimators=100, # default 100\n",
    "                            max_depth=None,\n",
    "                            max_features='auto',\n",
    "                            bootstrap=False,\n",
    "                            oob_score=False)\n",
    "               )]\n",
    "\n",
    "s = StackingRegressor(estimators=estimators,\n",
    "                      final_estimator=xgb.XGBRegressor(\n",
    "                          objective='reg:squarederror', # 13.32%\n",
    "                          # enable_categorical=True, # df CAT to INT64 !\n",
    "                          # use_label_encoder=True,\n",
    "                          # max_depth=17,\n",
    "                          n_estimators=21, \n",
    "                          random_state=R_S, \n",
    "                          n_jobs=-1)\n",
    "                      )\n",
    "s.fit(X_train, np.log(y_train))\n",
    "predict_new = np.exp(s.predict(X_test))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6abbb3b2",
   "metadata": {},
   "source": [
    "# sub 28\n",
    "#       --<<<<<<<  RF + GB = XGBR 13.32%      <<<< 12.45%  >>>>--\n",
    "# n_estimators=20 - 12.45%\n",
    "# max_depth none, n_estimators=40\n",
    "# XGBR n_estimators=500 14.68%\n",
    "# start 14.68%\n",
    "\n",
    "start = datetime.now()\n",
    "estimators = [('r_f', RandomForestRegressor(n_estimators=100, \n",
    "                                            n_jobs=-1, \n",
    "                                            random_state=R_S,\n",
    "                                            max_features=10,\n",
    "                                            max_depth=18,\n",
    "                                            min_samples_split=7)\n",
    "               ),\n",
    "              \n",
    "              ('g_b', GradientBoostingRegressor(n_estimators=500, # new\n",
    "                                  max_features=12, # None\n",
    "                                  max_depth=8,  #6\n",
    "                                  random_state=R_S)\n",
    "               )]\n",
    "\n",
    "\n",
    "s = StackingRegressor(estimators=estimators,\n",
    "                      final_estimator=xgb.XGBRegressor(objective='reg:squarederror',\n",
    "                         n_estimators=20, \n",
    "                         random_state=R_S, \n",
    "                         n_jobs=-1)\n",
    "                      )\n",
    "s.fit(X_train, np.log(y_train))\n",
    "predict_new = np.exp(s.predict(X_test))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "26e644a9",
   "metadata": {},
   "source": [
    "# sub 29\n",
    "#       --<<<<<<<  RF + ET = XGBR 13.32%      <<<<  12.85% >>>>--\n",
    "estimators = [('r_f', RandomForestRegressor(n_estimators=100, \n",
    "                                            n_jobs=-1, \n",
    "                                            random_state=R_S,\n",
    "                                            max_features=10,\n",
    "                                            max_depth=18,\n",
    "                                            min_samples_split=7)\n",
    "               ),\n",
    "              \n",
    "              ('e_t', ExtraTreesRegressor(random_state=R_S,\n",
    "                            n_estimators=100, # default 100\n",
    "                            max_depth=None,\n",
    "                            max_features='auto',\n",
    "                            bootstrap=False,\n",
    "                            oob_score=False)\n",
    "               )]\n",
    "\n",
    "\n",
    "s = StackingRegressor(estimators=estimators,\n",
    "                      final_estimator=xgb.XGBRegressor(objective='reg:squarederror',\n",
    "                         n_estimators=50, \n",
    "                         random_state=R_S, \n",
    "                         n_jobs=-1)\n",
    "                      )\n",
    "s.fit(X_train, np.log(y_train))\n",
    "predict_new = np.exp(s.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1682bc",
   "metadata": {},
   "source": [
    "#### Итоги  \n",
    "Материалы модуля отработаны за исключением парсинга. К сожалению не хватило времени освоить NLP.\n",
    "Задача отработана на обучающем датасете с ценой `all_auto_ru_09_09_2020.csv`.  \n",
    "\n",
    "Удалось минимизировать заданную метрику (MAPE) - 12.48% на датасете из бейслайна и 12.21% на Кагле. Доволен полученным 44 местом в лидерборде. В прошлых проектах мне это не удавалось."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc640de4",
   "metadata": {},
   "source": [
    "<a name=\"links\"></a>  \n",
    "### [LINKS](#coders)   \n",
    "+ #### [метрика MAPE](#mape)\n",
    "+ #### [CatBoost](#catb)\n",
    "+ #### [XGBoost](#x)\n",
    "+ #### [encoding](#enc)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c4cc47",
   "metadata": {},
   "source": [
    "<a name=\"mape\"></a>  \n",
    "[Wiki article about MAPE](http://en.wikipedia.org/wiki/Mean_absolute_percentage_error) \n",
    "[sklearn.metrics.mean_absolute_percentage_error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_percentage_error.html?highlight=mape)   \n",
    "Note here that **the output is not a percentage in the range `[0, 100]`** and a value of 100 does not mean 100% but 1e2.  \n",
    "[scikit-learn user guide - Mean absolute percentage error](https://scikit-learn.org/stable/modules/model_evaluation.html#mean-absolute-percentage-error)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a6e9f1",
   "metadata": {},
   "source": [
    "<a name=\"catb\"></a>  \n",
    "#### [Using Grid Search to Optimise CatBoost Parameters](https://effectiveml.com/using-grid-search-to-optimise-catboost-parameters.html)\n",
    "#### [Hyperparameter Tuning and Optimization(CatBoost)](https://medium.com/aiplusoau/hyperparameter-tuning-a5fe69d2a6c7)\n",
    "[CatBoost от Яндекса. Разбираемся](https://xn--80ajickj6abfedo.xn--p1ai/2019/04/29/catboost-%D0%BE%D1%82-%D1%8F%D0%BD%D0%B4%D0%B5%D0%BA%D1%81%D0%B0-%D1%80%D0%B0%D0%B7%D0%B1%D0%B8%D1%80%D0%B0%D0%B5%D0%BC%D1%81%D1%8F/)  \n",
    "[Как CatBoost работает с категориальными фичами ](https://catboost.ai/en/docs/concepts/algorithm-main-stages_cat-to-numberic)\n",
    "[Перевод Быстрый градиентный бустинг с CatBoost](http://personeltest.ru/article-6033-perevod-bystryy-gradientnyy-busting-s-catboost.html)   \n",
    "[CatBoost от Яндекса - for gradient boosting on decision trees](https://catboost.ai/)\n",
    "[Python package installation from Catboost off site](https://catboost.ai/en/docs/concepts/python-installation)   \n",
    "[conda install](https://catboost.ai/en/docs/installation/python-installation-method-conda-install)  \n",
    "[anaconda / packages / catboost 0.26.1](https://anaconda.org/anaconda/catboost) \n",
    "**To install this package with conda run:**   \n",
    "`conda install -c anaconda catboost`   \n",
    "[CatBoostRegressor usage examples](https://catboost.ai/en/docs/concepts/python-usages-examples)  \n",
    "#### [CatBoostRegressor FIT parameters list](https://catboost.ai/en/docs/concepts/python-reference_catboostregressor_fit)\n",
    "[CatBoost Regressor general setting](https://catboost.ai/en/docs/concepts/python-reference_catboostregressor)\n",
    "[Usage of class_weights in catboostclassifier](https://stackoverflow.com/questions/57565510/usage-of-class-weights-in-catboostclassifier/62832370#62832370)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e3cf31",
   "metadata": {},
   "source": [
    "#### [Comparing machine learning models for a regression problem](https://dibyendudeb.com/comparing-machine-learning-regression-models-using-python/#Application_of_Support_Vector_Regression) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0504af6",
   "metadata": {},
   "source": [
    "три популярных метода бустинга, отличия которых хорошо донесены в статье \n",
    "####  [CatBoost vs. LightGBM vs. XGBoost](https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6eb7da",
   "metadata": {},
   "source": [
    "**XGBoost** <a name=\"x\"></a>  [Trouble training XGBoost on categorical column](https://stackoverflow.com/questions/56088264/trouble-training-xgboost-on-categorical-column)  \n",
    "#### [off doc - Categorical Data](https://xgboost.readthedocs.io/en/latest/tutorials/categorical.html)   \n",
    "ValueError: Experimental support for categorical data is not implemented for current tree method yet.  \n",
    "[conda-forge / packages / xgboost](https://anaconda.org/conda-forge/xgboost)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce94bd4",
   "metadata": {},
   "source": [
    "**[Natural Language Processing With Python's NLTK Package](https://realpython.com/nltk-nlp-python/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552b1817",
   "metadata": {},
   "source": [
    "**Ecoding**  <a name=\"enc\"></a>  [Can anyone explain me StandardScaler?](https://stackoverflow.com/questions/40758562/can-anyone-explain-me-standardscaler)  \n",
    "[Standardization and Min-Max scaling](https://sebastianraschka.com/Articles/2014_about_feature_scaling.html#standardization-and-min-max-scaling)  \n",
    "[Difference between OrdinalEncoder and LabelEncoder](https://datascience.stackexchange.com/questions/39317/difference-between-ordinalencoder-and-labelencoder/64177#comment69359_64177)   \n",
    "[How can I one hot encode in Python?](https://stackoverflow.com/questions/37292872/how-can-i-one-hot-encode-in-python)  \n",
    " [Why Using Mean Squared Error(MSE) Cost Function for Binary Classification is a Bad Idea](https://towardsdatascience.com/why-using-mean-squared-error-mse-cost-function-for-binary-classification-is-a-bad-idea-933089e90df7)   \n",
    "[t-SNE - t-distributed Stohastic Neighbor Embedding](https://habr.com/ru/company/ods/blog/323210/)   \n",
    "[original article - Guide to Encoding Categorical Values in Python](https://pbpython.com/categorical-encoding.html)   \n",
    "[перевод статьи по кодированию категориальных значений ](https://dfedorov.spb.ru/pandas/%D0%A0%D1%83%D0%BA%D0%BE%D0%B2%D0%BE%D0%B4%D1%81%D1%82%D0%B2%D0%BE%20%D0%BF%D0%BE%20%D0%BA%D0%BE%D0%B4%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D1%8E%20%D0%BA%D0%B0%D1%82%D0%B5%D0%B3%D0%BE%D1%80%D0%B8%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D1%85%20%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D0%B9%20%D0%B2%20Python.html)  \n",
    "[Encoding Categorical  - GIT notebook](https://github.com/chris1610/pbpython/blob/master/notebooks/Category-Encoding-Article.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5269ca6f",
   "metadata": {},
   "source": [
    "**column transformer**   \n",
    "[sklearn.compose.make_column_transformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html)  \n",
    "[How to Use the ColumnTransformer for Data Preparation](https://machinelearningmastery.com/columntransformer-for-numerical-and-categorical-data/)  \n",
    "[How to use sklearn Column Transformer?](https://stackoverflow.com/questions/54160370/how-to-use-sklearn-column-transformer)  \n",
    "[Easier Machine Learning with the New Column Transformer from Scikit-Learn](https://medium.com/vickdata/easier-machine-learning-with-the-new-column-transformer-from-scikit-learn-c2268ea9564c)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603d5f30",
   "metadata": {},
   "source": [
    "#### список литературы    <a name=\"lit\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cee3f1",
   "metadata": {},
   "source": [
    "+ Джейк ВандерПЛАС Python для сложных задач: наука о данных и машинное обучение 2018\n",
    "+ ##### [Jake VanderPLAS Python Data Science Handbook (notebook on his site)](https://jakevdp.github.io/PythonDataScienceHandbook/)  \n",
    "+ ##### [Jake VanderPLAS Python Data Science Handbook (GIT notebook)](https://github.com/jakevdp/PythonDataScienceHandbook/tree/master/notebooks)  \n",
    "+ Aurélien Géron Hands-On Machine Learning with Scikit-Learn and TensorFlow    \n",
    "+ Орельен Жерон Прикладное машинное обучение с помощью Scikit-Learn и TensorFlow  \n",
    "+ Alice Zheng & Amanda Casari  Feature Engineering for Machine Learning\n",
    "+ ##### [Alice Zheng & Amanda Casari  FE for ML - GIT notebooks](https://github.com/alicezheng/feature-engineering-book)   \n",
    "+ Wes McKINNEY  Python for Data Analysis  \n",
    "+ ##### [Wes McKINNEY  Python for Data Analysis - GIT notebook](https://github.com/wesm/pydata-book)  \n",
    "+ Andreas MULLER & Sarah GUIDO  Introduction to Machine Learning with Python A Guide for Data Scientists   \n",
    "+ ##### [MULLER -  book code on GIT](https://github.com/amueller/introduction_to_ml_with_python)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5277fba",
   "metadata": {},
   "source": [
    "### END  <a name=\"end\"></a>  \n",
    "## [back to TOP](#top)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
